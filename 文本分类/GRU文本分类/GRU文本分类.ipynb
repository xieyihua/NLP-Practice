{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d692e1-7445-4f93-8d50-f0026886371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\pytorch1.7-py3.8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchtext \n",
    "from torchtext import data\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c96830-2bc3-4d54-9204-9c0ca2c2397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Classfier_Data():\n",
    "    def __init__(self):\n",
    "        self.path = os.path.abspath('.')\n",
    "        self.df = pd.read_csv(os.path.join(self.path,'data\\ChnSentiCorp_htl_all.csv'))\n",
    "        self.df['review'] = self.df['review'].astype(str)\n",
    "    def data_info(self):\n",
    "        print(self.df.info())\n",
    "    def distri_label(self):\n",
    "        print(self.df['label'].value_counts())\n",
    "    def clear_content(self):\n",
    "#         pattern = r'''[!\\\"#$%&'()*+,-./:;<=>?@[\\\\\\]^_`{|}~—！，。？·￥、《》···【】：\" \"''\\s0-9]+'''\n",
    "#         re_obj = re.compile(pattern)\n",
    "        patten = r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\\\\\]^_`{|}~—！，。？·￥、《》···【】：\" \"''\\s0-9]+\"  \n",
    "        re_obj = re.compile(patten)\n",
    "\n",
    "        # 替换函数--去除标点符号和数字\n",
    "        def clear(text):\n",
    "            return re_obj.sub('', text)\n",
    "\n",
    "        # 将正则表达式替换函数应用于每一行\n",
    "        self.df[\"review\"] = self.df[\"review\"].apply(clear)\n",
    "#         self.df['review'] = self.df['review'].apply(lambda x:re_obj.sub('',x))\n",
    "    def cut_sentence(self):\n",
    "        self.df['review'] = self.df['review'].apply(lambda x:jieba.lcut(x))\n",
    "    def remove_stoplist(self):\n",
    "        stop_words = \"data/stoplist.txt\"\n",
    "        stoplist = [i.strip() for i in open(stop_words, encoding='utf-8').readlines()]  #读取停用词列表\n",
    "        self.df['review'] = self.df['review'].apply(lambda x:' '.join([word for word in x if word not in stoplist]))\n",
    "    def get_text_vector(self,min_df,max_df,max_features):\n",
    "        self.tfidf = TfidfVectorizer(min_df = min_df,max_df = max_df,max_features = max_features)\n",
    "        self.tfidf.fit(self.df['review'])\n",
    "        text = self.tfidf.transform(self.df['review'])\n",
    "        return text\n",
    "    def find_optimal_clusters(self,text,max_k):\n",
    "        iters = range(2,max_k+1,2)\n",
    "        sse = []\n",
    "        for k in iters:\n",
    "            km = MiniBatchKMeans(n_clusters = k,init_size = 1024,batch_size = 2048,random_state = 20).fit(text)\n",
    "            sse.append(km.inertia_)\n",
    "            print('Fit {} clusters'.format(k))\n",
    "        f,ax = plt.subplots(1,1)\n",
    "        ax.plot(iters,sse,marker = 'o')\n",
    "        ax.set_xlabel('Cluster Centers')\n",
    "        ax.set_xticks(iters)\n",
    "        ax.set_xticklabels(iters)\n",
    "        ax.set_ylabel('SSE')\n",
    "        ax.set_title('SSE by Cluster Plot')\n",
    "    def plot_tnse_pca(self,text,cluster):\n",
    "        max_label = max(cluster)\n",
    "        max_items = np.random.choice(range(text.shape[0]),size = 3000,replace = False)\n",
    "        tdata = np.asarray(text[max_items,:].todense())\n",
    "        pca = PCA(n_components = 2).fit_transform(tdata)\n",
    "        tsne = TSNE().fit_transform(PCA(n_components = 50).fit_transform(tdata))\n",
    "        \n",
    "        \n",
    "        idx = np.random.choice(range(pca.shape[0]),size = 300,replace = False)\n",
    "        label_subset = cluster[max_items]\n",
    "        label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "        \n",
    "        \n",
    "        f,ax = plt.subplots(1,2,figsize = (14,8))\n",
    "        ax[0].scatter(pca[idx,0],pca[idx,1],c = label_subset)\n",
    "        ax[0].set_title('PCA Cluster Plot')\n",
    "        \n",
    "        ax[1].scatter(tsne[idx,0],tsne[idx,1],c = label_subset)\n",
    "        ax[1].set_title('TSNE Cluster Plot')\n",
    "    def get_top_keywords(self,text,clusters,labels,n_terms):\n",
    "        df = pd.DataFrame(text.todense()).groupby(clusters).mean()\n",
    "        for i,r in df.iterrows():\n",
    "            print('\\n Cluster {}'.format(i))\n",
    "            print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "    def replace_word(self,sentence):\n",
    "        wordlist = []\n",
    "        wordlist += [word.strip() for word in open('data/负面情感词语.txt',encoding = 'gbk').readlines()]\n",
    "        wordlist += [word.strip() for word in open('data/负面评价词语.txt',encoding = 'gbk').readlines()]\n",
    "        L = len(wordlist) - 1\n",
    "        flag = 1\n",
    "        sentence_tag = jieba.posseg.cut(sentence.strip())\n",
    "        ans = ''\n",
    "        for i,w in enumerate(sentence_tag):\n",
    "            if w.flag != 'a':\n",
    "                ans += w.word\n",
    "            else:\n",
    "                index = random.randint(1,L)\n",
    "                ans += wordlist[index]\n",
    "                ans += ''.join([w.word for j ,w in enumerate(sentence_tag) if j > i])\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag:\n",
    "            ans += ''.join([w.word for w in sentence_tag])\n",
    "            index = random.randint(1,L)\n",
    "            ans += wordlist[index]\n",
    "        return ans\n",
    "    def get_balance_distri(self):\n",
    "        df = self.df\n",
    "        neg_df = df[df['label'] == 0].copy(deep = True)\n",
    "        neg_df['review'] = neg_df['review'].apply(lambda x:self.replace_word(x))\n",
    "        con_df = pd.concat([self.df,neg_df])\n",
    "        return con_df\n",
    "    def split_data(self,df,split = 8000):\n",
    "        self.train_df,self.valid_df,self.test_df = df[:split],df[split:split + 1000],df[split+1000:]\n",
    "    def store_data(self):\n",
    "        self.train_df[['review','label']].to_csv('data/train.csv',index = False)\n",
    "        self.valid_df[['review','label']].to_csv('data/valid.csv',index = False)\n",
    "        self.test_df[['review','label']].to_csv('data/test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d83d0f-428b-4846-85c3-e950f3a258ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\m1824\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7766 entries, 0 to 7765\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   7766 non-null   int64 \n",
      " 1   review  7766 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 121.5+ KB\n",
      "None\n",
      "1    5322\n",
      "0    2444\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.289 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 2 clusters\n",
      "Fit 4 clusters\n",
      "Fit 6 clusters\n",
      "Fit 8 clusters\n",
      "Fit 10 clusters\n",
      "Fit 12 clusters\n",
      "Fit 14 clusters\n",
      "Fit 16 clusters\n",
      "Fit 18 clusters\n",
      "Fit 20 clusters\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    td = Text_Classfier_Data()\n",
    "    td.data_info()\n",
    "    td.distri_label()\n",
    "    td.clear_content()\n",
    "    td.cut_sentence()\n",
    "    td.remove_stoplist()\n",
    "    text = td.get_text_vector(5,0.95,8000)\n",
    "    td.find_optimal_clusters(text,20)\n",
    "    clusters = MiniBatchKMeans(n_clusters = 5,init_size = 1024,batch_size = 2048,random_state = 20).fit_predict(text)\n",
    "    td.plot_tnse_pca(text,clusters)\n",
    "    td.get_top_keywords(text,clusters,td.tfidf.get_feature_names_out(),10)\n",
    "    km = MiniBatchKMeans(n_clusters = 5,init_size = 1024,batch_size = 1024,random_state = 20).fit(text)\n",
    "    td.df['cluster'] = km.labels_\n",
    "    td.df['label'].value_counts()\n",
    "    td.df[td.df['label'] == 0].value_counts()\n",
    "    con_df = td.get_balance_distri()\n",
    "    print('\\n------------new data distribution---------\\n')\n",
    "    print(con_df['label'].value_counts())\n",
    "    td.split_data(con_df)\n",
    "    td.store_data()\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80c5f2-8b6a-44a3-b44f-388d6e02a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader():\n",
    "    tokenize = lambda x:x.split()\n",
    "    REVIEW = data.Field(sequential = True,tokenize = tokenize,fix_length = 256)\n",
    "    LABEL = data.LabelField(sequential = False,use_vocab = False)\n",
    "\n",
    "    train_data,valid_data,test_data = data.TabularDataset.splits(\n",
    "                                     path = 'data',\n",
    "                                     train = 'train.csv',\n",
    "                                     validation = 'valid.csv',test = 'test.csv',\n",
    "                                     format = 'csv',\n",
    "                                     fields = [('review',REVIEW),('label',LABEL)],\n",
    "                                     skip_header = True)\n",
    "    return REVIEW,LABEL,train_data,valid_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4406e-8990-4d4f-a4cd-0d811999987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW,LABEL,train_data,valid_data,test_data = DataLoader()\n",
    "REVIEW.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10802d2c-8cdd-4d01-bd33-09b5c2b4f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter,val_iter,test_iter = data.BucketIterator.splits((train_data,valid_data,test_data),\n",
    "                                                    batch_size = 64,\n",
    "                                                    sort = False,\n",
    "                                                    sort_within_batch = False,repeat = False)\n",
    "def getTEXT():\n",
    "    return REVIEW\n",
    "def getLABEL():\n",
    "    return LABEL\n",
    "def getIter():\n",
    "    return train_iter,val_iter,test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f750924-8a90-4a0d-8cf9-d8f2341bd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(test_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8796ea-d45d-4ed3-9328-80f318067863",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b8f2f-3306-49ec-989a-8be6788ba376",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "class TextRNN_Attention(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(TextRNN_Attention, self).__init__()\n",
    "        Vocab = len(getTEXT().vocab)  ## 已知词的数量\n",
    "        Dim = 256  ##每个词向量长度\n",
    "        dropout = 0.2\n",
    "        hidden_size = 256 #隐藏层数量\n",
    "        num_classes = args.class_num ##类别数\n",
    "        num_layers = 2 ##双层LSTM\n",
    "\n",
    "        self.embedding = nn.Embedding(Vocab, Dim)  ## 词向量，这里直接随机\n",
    "        self.lstm = nn.LSTM(Dim, hidden_size, num_layers,\n",
    "                            bidirectional = True, batch_first = True, dropout = args.dropout)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch len, text size]\n",
    "        x = self.embedding(x)\n",
    "        # [batch size, text size, embedding]\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        # output = [batch size, text size, num_directions * hidden_size]\n",
    "        M = self.tanh1(output)\n",
    "        # [batch size, text size, num_directions * hidden_size]\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)\n",
    "        # [batch size, text size, 1]\n",
    "        out = output * alpha\n",
    "        # [batch size, text size, num_directions * hidden_size]\n",
    "        out = torch.sum(out, 1)\n",
    "        # [batch size, num_directions * hidden_size]\n",
    "        out = F.relu(out)\n",
    "        # [batch size, num_directions * hidden_size]\n",
    "        out = self.fc(out)\n",
    "        # [batch size, num_classes]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43cbb5-60c0-4f74-bba7-c5e5f9057fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.class_num = 2\n",
    "        self.fix_length = 256\n",
    "        self.batch_size = 256\n",
    "        # data label list\n",
    "        self.label_list = ['好', '差']\n",
    "        class_number = len(self.label_list)\n",
    "        # train details\n",
    "        self.epochs = ６\n",
    "        self.learning_rate = 1e-4\n",
    "        self.dropout = 0.2\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d4a31-3ff3-4e34-8552-89ee0c4ce001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_iter, name, device,args):\n",
    "    model = torch.load('done_model/'+name+'_model.pkl')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_test_num = len(test_iter.dataset)\n",
    "    for batch in test_iter:\n",
    "        feature = batch.review\n",
    "        target = batch.label\n",
    "        with torch.no_grad():\n",
    "            feature = torch.t(feature)\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "        out = model(feature)\n",
    "        loss = F.cross_entropy(out, target)\n",
    "        total_loss += loss.item()\n",
    "        accuracy += (torch.argmax(out, dim=1)==target).sum().item()\n",
    "        y_true.extend(target.cpu().numpy())\n",
    "        y_pred.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "    print('>>> Test loss:{}, Accuracy:{} \\n'.format(total_loss/total_test_num, accuracy/total_test_num))\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    print(score)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, y_pred, target_names = args.label_list, digits=3))\n",
    "\n",
    "def train_model(train_iter, dev_iter, model, name, device,args):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 25], gamma=0.6)\n",
    "    model.train()\n",
    "    best_acc = 0\n",
    "    print('training...')\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        accuracy = 0\n",
    "        total_train_num = len(train_iter.dataset)\n",
    "        progress_bar = tqdm(enumerate(train_iter), total=len(train_iter))\n",
    "        for i,batch in progress_bar:\n",
    "            feature = batch.review\n",
    "            target = batch.label\n",
    "            with torch.no_grad():\n",
    "                feature = torch.t(feature)\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(feature)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            accuracy += (torch.argmax(logit, dim=1) == target).sum().item()\n",
    "            progress_bar.set_description(\n",
    "            f'loss: {loss.item():.3f}')\n",
    "        print('>>> Epoch_{}, Train loss is {}, Accuracy:{} \\n'.format(epoch,loss.item()/total_train_num, accuracy/total_train_num))\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        accuracy = 0\n",
    "        total_valid_num = len(dev_iter.dataset)\n",
    "        progress_bar = tqdm(enumerate(dev_iter), total=len(dev_iter))\n",
    "        for i, batch in progress_bar:\n",
    "            feature = batch.review  # (W,N) (N)\n",
    "            target = batch.label\n",
    "            with torch.no_grad():\n",
    "                feature = torch.t(feature)\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "            out = model(feature)\n",
    "            loss = F.cross_entropy(out, target)\n",
    "            total_loss += loss.item()\n",
    "            accuracy += (torch.argmax(out, dim=1)==target).sum().item()\n",
    "        print('>>> Epoch_{}, Valid loss:{}, Accuracy:{} \\n'.format(epoch, total_loss/total_valid_num, accuracy/total_valid_num))\n",
    "        if(accuracy/total_valid_num > best_acc):\n",
    "            print('save model...')\n",
    "            best_acc = accuracy/total_valid_num\n",
    "            saveModel(model, name=name)\n",
    "\n",
    "def saveModel(model,name):\n",
    "    torch.save(model, 'done_model/'+name+'_model.pkl')\n",
    "\n",
    "name = 'TextRNN_Attention'\n",
    "model = TextRNN_Attention(args)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iter, val_iter, test_iter = getIter()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model(train_iter, val_iter, model, name, device,args)\n",
    "    test_model(test_iter, name, device,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca99f93-02fe-4b90-b9f2-33154744a63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432e362-cbcc-44db-9b05-dc60dfbf5080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytroch1.7-py3.8",
   "language": "python",
   "name": "pytroch1.7-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
